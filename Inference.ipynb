{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O97H3pCEvao4",
        "outputId": "1e97200b-fee7-419e-9979-015b9a0dc746"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from nltk import word_tokenize,sent_tokenize\n",
        "import nltk\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from tqdm import tqdm\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install Keras-Preprocessing\n",
        "\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/ai-forever/sbert_large_nlu_ru"
      ],
      "metadata": {
        "id": "fFJiEdJJv8NL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23343fe1-bf34-4e5a-eaf4-5b421bd452ac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m119.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Keras-Preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from Keras-Preprocessing) (1.22.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from Keras-Preprocessing) (1.16.0)\n",
            "Installing collected packages: Keras-Preprocessing\n",
            "Successfully installed Keras-Preprocessing-1.1.2\n",
            "Error: Failed to call git rev-parse --git-dir: exit status 128 \n",
            "Git LFS initialized.\n",
            "Cloning into 'sbert_large_nlu_ru'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 40 (delta 6), reused 0 (delta 0), pack-reused 28\u001b[K\n",
            "Unpacking objects: 100% (40/40), 604.71 KiB | 2.65 MiB/s, done.\n",
            "Filtering content: 100% (2/2), 3.18 GiB | 92.01 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_json(\"/content/drive/MyDrive/NLP_2023_mag/Contur/nlp_test_task_2023/nlp_test_task_2023/dataset/test.json\")"
      ],
      "metadata": {
        "id": "dupCke4pvn9v"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\""
      ],
      "metadata": {
        "id": "GmnzkZlRxZdP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertConfig,BertForQuestionAnswering\n",
        "from transformers import BertTokenizerFast\n",
        "\n",
        "tokenizer = BertTokenizerFast(vocab_file = \"/content/sbert_large_nlu_ru/vocab.txt\",do_lower_case=False)\n",
        "\n",
        "config = BertConfig.from_pretrained('/content/sbert_large_nlu_ru/config.json')\n",
        "\n",
        "state=torch.load(\"/content/drive/MyDrive/NLP_2023_mag/Contur/nlp_test_task_2023/nlp_test_task_2023/Models/Bert_QA_1.pt\")\n",
        "model = BertForQuestionAnswering.from_pretrained(pretrained_model_name_or_path=None, state_dict=state['model'], config=config)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "ZdJGbq5kvv1w"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X393r1Ldx5uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qeDdkiNm1hrU"
      },
      "outputs": [],
      "source": [
        "def tokenize_sen(json_data,t = \"Train\"):\n",
        "    text = json_data['text']\n",
        "    encoding = tokenizer(text,add_special_tokens=False)[0]\n",
        "\n",
        "    if t == \"Train\":\n",
        "        # Извекаем начало, конец и лэйбл\n",
        "        start_sen = json_data['entities'][0]['start']\n",
        "        end_sen = json_data['entities'][0]['end']\n",
        "        label = json_data['entities'][0]['label']\n",
        "        if end_sen==0:\n",
        "            return encoding, 0, 0, label\n",
        "        else:\n",
        "            # Извлекаем номера токенов, которые соответствует start_sen и end_sen-1\n",
        "            start_token = encoding.char_to_token(start_sen)\n",
        "            end_token = encoding.char_to_token(end_sen-1)\n",
        "\n",
        "            if start_token!=None and end_token!=None:\n",
        "                return encoding, start_token, end_token, label\n",
        "            else:\n",
        "                return None,None, None,None\n",
        "    elif t == \"Test\":\n",
        "        # На тесте извлекать start и end не нужно\n",
        "        label = json_data['entities'][0]['label']\n",
        "        return encoding, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "avsmHqS4miVH"
      },
      "outputs": [],
      "source": [
        "def tokenize_sens(json_datas,t = \"Train\"):\n",
        "    enc_st_et = []\n",
        "    if t == \"Train\":\n",
        "        for json_data in json_datas:\n",
        "            d = {}\n",
        "            # Токенизируем текст и получем его encoding, start, end и label\n",
        "            e, st, et, lab = tokenize_sen(json_data,t)\n",
        "            if st!=None and e!=None and et!=None:\n",
        "                d[\"encoding\"] = e\n",
        "                d[\"start_token\"] = st\n",
        "                d[\"end_token\"] = et\n",
        "                d[\"label\"] = lab\n",
        "                enc_st_et.append(d)\n",
        "            \n",
        "                \n",
        "    elif t == \"Test\":\n",
        "        for json_data in json_datas:\n",
        "            d = {}\n",
        "            e,lab = tokenize_sen(json_data,t)\n",
        "            d[\"encoding\"] = e\n",
        "            d[\"label\"] = lab\n",
        "            enc_st_et.append(d)\n",
        "        \n",
        "    return enc_st_et"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VAYx6EHBqbPo"
      },
      "outputs": [],
      "source": [
        "# Инициализируем индексы cls, pad и sep\n",
        "CLS_ID = tokenizer.cls_token_id\n",
        "PAD_ID = tokenizer.pad_token_id\n",
        "SEP_ID  = tokenizer.sep_token_id\n",
        "\n",
        "def cut_long_sequence(data_dict,t = \"Train\"):\n",
        "    # Токенизируем запрос\n",
        "    enc = tokenizer(data_dict['label'],add_special_tokens=False)[0]\n",
        "    len_query = len(enc.ids)\n",
        "    # Формируем структуру данных под Train, Test\n",
        "    if t == \"Train\":\n",
        "        sequences = {\n",
        "            \"input_ids\":[],\n",
        "            \"attention_mask\":[],\n",
        "            \"start_token\":[0],\n",
        "            \"end_token\":[0],\n",
        "            \"segment_ids\":[],\n",
        "            \"shift\":[]\n",
        "        }\n",
        "    elif t == \"Test\":\n",
        "        sequences = {\n",
        "            \"input_ids\":[],\n",
        "            \"attention_mask\":[],\n",
        "            \"segment_ids\":[],\n",
        "            \"shift\":[]\n",
        "        }\n",
        "    \n",
        "    # Получаем ids и attention_mask\n",
        "    input_ids = data_dict['encoding'].ids\n",
        "    att_ids = data_dict['encoding'].attention_mask\n",
        "    # Если len(input_ids)>=510-len_query, то обрезаем наши айди текста и attention_mask к нему\n",
        "    if len(input_ids)<510-len_query:\n",
        "        pass\n",
        "    else:\n",
        "        input_ids = input_ids[0:(510-len_query)]\n",
        "        att_ids = att_ids[0:(510-len_query)]\n",
        "    \n",
        "    # Длина padding\n",
        "    pad_len = 510 - len(input_ids) - len_query\n",
        "\n",
        "    # Формируем segment_ids, 0 - вопрос, 1 - текст\n",
        "    num_seg_a = len([CLS_ID]+enc.ids+[SEP_ID])\n",
        "    num_seg_b = len(input_ids + [PAD_ID] * pad_len)\n",
        "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "\n",
        "    # Формируем input_ids и attention_mask нужной длины\n",
        "    input_ids = [CLS_ID]+enc.ids+[SEP_ID]+input_ids + [PAD_ID] * pad_len\n",
        "    attention_mask = [1] +enc.attention_mask+[1]+att_ids +[PAD_ID] * pad_len\n",
        "\n",
        "    #Инициализируем структуру данных\n",
        "    sequences['input_ids'] = input_ids\n",
        "    sequences['attention_mask'] = attention_mask\n",
        "    sequences['segment_ids'] = segment_ids\n",
        "\n",
        "    if t == \"Train\":\n",
        "        sequences['start_token'] = [data_dict['start_token']+len([CLS_ID]+enc.ids+[SEP_ID])]\n",
        "        sequences['end_token'] = [data_dict['end_token']+len([CLS_ID]+enc.ids+[SEP_ID])]\n",
        "    sequences['shift'] = [len([CLS_ID]+enc.ids+[SEP_ID])]\n",
        "    return sequences, data_dict['encoding']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eCmnolAp1wWw"
      },
      "outputs": [],
      "source": [
        "def preprosses(data,tp = \"Train\"):\n",
        "    encodings = []\n",
        "    if tp == \"Train\":\n",
        "        sequences = {\n",
        "            \"input_ids\":[],\n",
        "            \"attention_mask\":[],\n",
        "            \"start_token\":[],\n",
        "            \"end_token\":[],\n",
        "            \"segment_ids\":[],\n",
        "            \"shift\":[]\n",
        "        }\n",
        "        cols = [\"input_ids\", \"attention_mask\", \"start_token\", \"end_token\", \"segment_ids\",\"shift\"]\n",
        "    elif tp == \"Test\":\n",
        "        sequences = {\n",
        "            \"input_ids\":[],\n",
        "            \"attention_mask\":[],\n",
        "            \"segment_ids\":[],\n",
        "            \"shift\":[]\n",
        "        }\n",
        "        cols = [\"input_ids\", \"attention_mask\", \"segment_ids\",\"shift\"]\n",
        "\n",
        "    # Токенизируем данные\n",
        "    enc_st_et = tokenize_sens(data,tp)\n",
        "\n",
        "    for i in range(len(enc_st_et)):\n",
        "        # Формируем данные под QA модель, обрезаем длинные последовательности\n",
        "        sequence,enc = cut_long_sequence(enc_st_et[i],tp)\n",
        "        encodings.append(enc)\n",
        "        for t in cols:\n",
        "            sequences[t].append(sequence[t])\n",
        "    \n",
        "    for t in cols:\n",
        "        sequences[t] = np.array(sequences[t])\n",
        "    return sequences,encodings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,data_input, data_att, segment_ids,shift):\n",
        "        self.data_input = data_input\n",
        "        self.data_att = data_att\n",
        "        self.segment_ids = segment_ids\n",
        "        self.shift = shift\n",
        "    def __len__(self):\n",
        "        return len(self.data_input)\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        return torch.from_numpy(self.data_input[idx]),torch.from_numpy(self.data_att[idx]),torch.from_numpy(self.segment_ids[idx]),torch.from_numpy(self.shift[idx])"
      ],
      "metadata": {
        "id": "22du32eO1s5c"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_answer(input_label, input_text):\n",
        "    d = {'label': input_label,'text': input_text}\n",
        "    test_df = pd.DataFrame(data=d)\n",
        "\n",
        "    data_test = []\n",
        "    for i in range(len(test_df)):\n",
        "        data = {}\n",
        "        data['text'] = test_df['text'][i]\n",
        "        data['entities'] = []\n",
        "        lab = test_df['label'][i]\n",
        "\n",
        "        data['entities'].append({\"label\":lab})\n",
        "        data_test.append(data)\n",
        "\n",
        "        sequences_test,encodings_test = preprosses(data_test,tp=\"Test\")\n",
        "\n",
        "    test_segment_ids = []\n",
        "    test_attention_mask = []\n",
        "    test_input_ids = []\n",
        "    test_shift = []\n",
        "    for i in range(len(sequences_test['input_ids'])):\n",
        "        test_attention_mask.append(sequences_test['attention_mask'][i])\n",
        "        test_input_ids.append(sequences_test['input_ids'][i])\n",
        "        test_segment_ids.append(sequences_test['segment_ids'][i])\n",
        "        test_shift.append(sequences_test['shift'][i])\n",
        "\n",
        "    test_segment_ids = np.array(test_segment_ids)\n",
        "    test_attention_mask = np.array(test_attention_mask)\n",
        "    test_input_ids = np.array(test_input_ids)\n",
        "    test_shift = np.array(test_shift)\n",
        "\n",
        "    test_dataset = TestDataset(test_input_ids, test_attention_mask, test_segment_ids,test_shift)\n",
        "    test_dataloader = torch.utils.data.DataLoader(test_dataset,shuffle=False,batch_size=4,num_workers=2,pin_memory=True)\n",
        "\n",
        "\n",
        "    model.eval()#переводим сеть в состояние тестирования\n",
        "    eval_loss=0\n",
        "    predicted_start = []\n",
        "    predicted_end = []\n",
        "    shifts = []\n",
        "\n",
        "    with torch.no_grad():#не вычисляем параметры\n",
        "        for batch_inputs, batch_att, batch_segment, shift in test_dataloader:\n",
        "\n",
        "            batch_inputs = batch_inputs.to(device) # переводим на cudu\n",
        "            batch_att = batch_att.to(device)\n",
        "            batch_segment = batch_segment.to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids = batch_inputs, attention_mask = batch_att,\n",
        "                token_type_ids = batch_segment\n",
        "                )\n",
        "            \n",
        "            predicted_start.extend(torch.argmax(outputs.start_logits,axis=-1).detach().cpu().numpy())\n",
        "            predicted_end.extend(torch.argmax(outputs.end_logits,axis=-1).detach().cpu().numpy())\n",
        "            shifts.extend(shift.numpy())\n",
        "\n",
        "    predicted_start_r = []\n",
        "    predicted_end_r = []\n",
        "\n",
        "    for i in range(len(predicted_end)):\n",
        "\n",
        "        if predicted_start[i] - shifts[i][0] < 0 or predicted_end[i]-shifts[i][0] < 0:\n",
        "            predicted_start_r.append(0)\n",
        "            predicted_end_r.append(0)\n",
        "        else:\n",
        "            if encodings_test[i].token_to_chars(predicted_start[i] - shifts[i][0]) != None and encodings_test[i].token_to_chars(predicted_end[i]-shifts[i][0])!= None:\n",
        "                if predicted_end[i] - shifts[i][0] == 0:\n",
        "                    predicted_start_r.append(0)\n",
        "                    predicted_end_r.append(0)\n",
        "                else: \n",
        "                    start,_ = encodings_test[i].token_to_chars(predicted_start[i] - shifts[i][0])\n",
        "                    _,end = encodings_test[i].token_to_chars(predicted_end[i] - shifts[i][0])\n",
        "                    predicted_start_r.append(start)\n",
        "                    predicted_end_r.append(end)\n",
        "            else:\n",
        "                predicted_start_r.append(0)\n",
        "                predicted_end_r.append(0)\n",
        "\n",
        "    for i in range(len(predicted_start_r)):\n",
        "        if predicted_start_r[i]>=predicted_end_r[i]:\n",
        "            predicted_start_r[i]=0\n",
        "            predicted_end_r[i]=0\n",
        "\n",
        "    test_df[\"answer_start\"] = 0\n",
        "    test_df[\"answer_end\"] = 0\n",
        "    test_df[\"answer_text\"] = 0\n",
        "\n",
        "    # Заполняем таблица answer_start, answer_end, answer_text\n",
        "    for i in range(len(predicted_start_r)):\n",
        "        test_df['answer_start'][i]=predicted_start_r[i]\n",
        "        test_df['answer_end'][i]=predicted_end_r[i]\n",
        "        test_df['answer_text'][i]=data_test[i]['text'][predicted_start_r[i]:predicted_end_r[i]]\n",
        "\n",
        "    return test_df"
      ],
      "metadata": {
        "id": "WEmcPFqGKPmh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = test_df['text'][0:2]\n",
        "input_label = test_df['label'][0:2]\n",
        "predict_answer(input_label, input_text)"
      ],
      "metadata": {
        "id": "sxh8elBtM8b7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}